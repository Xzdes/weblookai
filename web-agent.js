// web-agent.js

import puppeteer from 'puppeteer';
import { load } from 'cheerio';

class WebAgent {
  constructor(options = {}) {
    this.options = {
      // --- ИЗМЕНЕНИЕ: Теперь это массив поисковых систем ---
      searchEngines: [
        'https://search.brave.com/search?q=',
        'https://duckduckgo.com/?q=',
        'https://www.startpage.com/sp/search?query='
      ],
      // ----------------------------------------------------
      maxResultsToVisit: 10,
      minSuccessfulSources: 5,
      minContentLength: 200,
      ...options,
    };
    this.browser = null;
    this.visitedUrls = new Set();
  }

  // --- Методы launch() и close() остаются без изменений ---

  async launch() {
    console.log('--- [Web Agent] Launching headless browser... ---');
    this.browser = await puppeteer.launch({
      headless: true,
      args: ['--no-sandbox', '--disable-setuid-sandbox'],
    });
  }

  async close() {
    if (this.browser) {
      console.log('--- [Web Agent] Closing browser... ---');
      await this.browser.close();
    }
  }

  /**
   * Main orchestration method. v2: Now iterates through multiple search engines.
   * @param {string[]} queries - The search queries generated by the LLM.
   * @returns {Promise<{context: string, report: object}>}
   */
  async investigate(queries) {
    if (!this.browser) throw new Error('Browser is not launched. Please call launch() first.');

    const primaryQuery = queries[0];
    console.log(`--- [Web Agent] Investigating main query: "${primaryQuery}" ---`);
    
    let allFoundUrls = [];
    
    // --- НОВЫЙ ЦИКЛ ПО ПОИСКОВИКАМ ---
    for (const searchEngineUrl of this.options.searchEngines) {
      console.log(`\n[Web Agent] Trying search engine: ${new URL(searchEngineUrl).hostname}`);
      const urls = await this.fetchInitialUrls(primaryQuery, searchEngineUrl);
      
      if (urls.length > 0) {
        console.log(`[Web Agent] Found ${urls.length} potential URLs. Proceeding with extraction.`);
        allFoundUrls = [...new Set([...allFoundUrls, ...urls])]; // Merge and deduplicate
        // Мы можем либо продолжать искать в других поисковиках для большего выбора,
        // либо остановиться здесь, если нашли достаточно. Давайте остановимся.
        break; 
      } else {
        console.log(`[Web Agent] No usable URLs found on this search engine. Trying next...`);
      }
    }
    
    if (allFoundUrls.length === 0) {
        console.log('--- [Web Agent] Could not find any URLs on any search engine.');
        return { context: '', report: { successful: 0, failed: 0, totalVisited: 0 } };
    }

    const { context, report } = await this.visitAndExtract(allFoundUrls.slice(0, this.options.maxResultsToVisit), queries);

    console.log(`--- [Web Agent] Investigation complete. Report: ${report.successful} successful, ${report.failed} failed out of ${report.totalVisited} sites visited.`);
    return { context, report };
  }

  /**
   * Fetches URLs. v4: Now takes search engine URL as an argument.
   * @param {string} query
   * @param {string} searchEngineUrl
   * @returns {Promise<string[]>}
   */
  async fetchInitialUrls(query, searchEngineUrl) {
    const page = await this.browser.newPage();
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36');
    await page.setViewport({ width: 1280, height: 800 });

    try {
      await page.goto(searchEngineUrl + encodeURIComponent(query), { waitUntil: 'domcontentloaded' });
      await page.waitForSelector('body', { timeout: 10000 });
      await new Promise(r => setTimeout(r, 3000));

      const allLinks = await page.evaluate(() => {
        return Array.from(document.querySelectorAll('a')).map(anchor => ({
          href: anchor.href,
          text: anchor.textContent.trim().toLowerCase(),
        }));
      });
      
      const keywords = query.toLowerCase().split(' ').filter(k => k.length > 2);
      const domainBlacklist = [
        'apple.com', 'google.com', 'facebook.com', 'twitter.com', 'instagram.com',
        'linkedin.com', 'reddit.com', 'duckduckgo.com', 'brave.com', 'microsoft.com',
        'amazon.com', 'wikipedia.org', 'youtube.com', 'torproject.org', 'startpage.com'
      ];
      
      const filteredLinks = allLinks.map(link => {
        try {
          const url = new URL(link.href);
          if (url.protocol !== 'http:' && url.protocol !== 'https:') return null;
          if (domainBlacklist.some(domain => url.hostname.includes(domain))) return null;
          const badPathSegments = ['login', 'register', 'signin', 'signup', 'cart', 'policy', 'about', 'contact'];
          if (badPathSegments.some(segment => url.pathname.includes(segment))) return null;
          let score = 0;
          const linkTextAndPath = (link.text + ' ' + url.pathname).toLowerCase();
          for (const keyword of keywords) {
            if (linkTextAndPath.includes(keyword)) score++;
          }
          if (url.pathname.includes('/blog/') || url.pathname.includes('/post/') || url.href.endsWith('.html')) score += 2;
          if (score < 1) return null;
          return { href: url.href, score };
        } catch (e) { return null; }
      }).filter(Boolean).sort((a, b) => b.score - a.score).map(link => link.href);

      return [...new Set(filteredLinks)];

    } catch (error) {
      console.error(`[Web Agent] -> FAILED to fetch from ${new URL(searchEngineUrl).hostname}: ${error.message}`);
      return [];
    } finally {
      await page.close();
    }
  }
  
  // --- Метод visitAndExtract() остается без изменений ---
  async visitAndExtract(urls, keywords) {
    let context = '';
    const report = { successful: 0, failed: 0, totalVisited: 0 };
    const keywordRegex = new RegExp(keywords.join('|').replace(/\s/g, '|'), 'i');

    for (const url of urls) {
      if (report.successful >= this.options.minSuccessfulSources) {
        console.log(`[Web Agent] Reached the minimum source count of ${this.options.minSuccessfulSources}. Stopping.`);
        break;
      }
      if (this.visitedUrls.has(url)) continue;

      this.visitedUrls.add(url);
      report.totalVisited++;
      
      const page = await this.browser.newPage();
      try {
        console.log(`[Web Agent] Visiting (${report.totalVisited}/${this.options.maxResultsToVisit}): ${url}`);
        await page.goto(url, { waitUntil: 'domcontentloaded', timeout: 15000 });
        const html = await page.content();
        const pageContent = this.extractMainContent(html);

        if (pageContent.length < this.options.minContentLength || !keywordRegex.test(pageContent)) {
          throw new Error('Content too short or not relevant.');
        }

        console.log(`[Web Agent] -> SUCCESS: Extracted ${pageContent.length} chars of relevant content.`);
        context += `--- Source: ${url} ---\n${pageContent}\n\n`;
        report.successful++;
      } catch (error) {
        console.log(`[Web Agent] -> FAILED: ${error.message}`);
        report.failed++;
      } finally {
        await page.close();
      }
    }
    return { context, report };
  }

  // --- Метод extractMainContent() остается без изменений ---
  extractMainContent(html) {
    const $ = load(html);
    $('nav, footer, header, aside, script, style, noscript, [role="navigation"], [role="banner"], [role="contentinfo"]').remove();
    let mainContent = $('main, article, .post, .content, .entry-content').first().text();
    if (!mainContent) {
      mainContent = $('body').text();
    }
    return mainContent.replace(/\s\s+/g, ' ').replace(/\n\s*\n/g, '\n').trim();
  }
}

export default WebAgent;